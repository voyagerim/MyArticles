import requests
import csv
import json
from io import StringIO
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import time
import datetime
import sys
import os
import argparse
import re
import pickle
from functools import lru_cache
import tempfile
import signal

# === –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∑–∞–ø—É—â–µ–Ω—ã –ª–∏ –º—ã –∏–∑ PyCharm ===
def is_running_in_pycharm():
    return 'PYCHARM_HOSTED' in os.environ or 'PYDEVD_LOAD_VALUES_ASYNC' in os.environ

# === –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ ===
parser = argparse.ArgumentParser(description='–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å Quarry –Ω–∞ –í–∏–∫–∏—Å–∫–ª–∞–¥')
parser.add_argument('-q', '--query-id', type=int, default=93243, help='ID –∑–∞–ø—Ä–æ—Å–∞ Quarry')
parser.add_argument('--test', action='store_true', help='–¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏')
parser.add_argument('-v', '--verbose', action='store_true', help='–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥')
parser.add_argument('-w', '--workers', type=int, default=10, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤')
parser.add_argument('--no-cache', action='store_true', help='–ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ')
# –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞ prose_size
parser.add_argument('--force-recalculate', action='store_true', help='–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Å—á–µ—Ç readable prose size')
parser.add_argument('--no-force-recalculate', action='store_true', help='–û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Å—á–µ—Ç readable prose size')
args = parser.parse_args()

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===
QUERY_ID = args.query_id
COMMONS_DATA_TITLE = "Data:Voyagerim/myarticles.tab"
COMMONS_API = "https://commons.wikimedia.org/w/api.php"
WIKIPEDIA_API = "https://ru.wikipedia.org/w/api.php"
USERNAME = "Voyagerim"
PASSWORD = "mailmans1980"
SUMMARY = "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Quarry"
LOGFILE = "upload_log.txt"
TEST_MODE = args.test
VERBOSE = args.verbose
MAX_WORKERS = args.workers
USE_CACHE = not args.no_cache

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ FORCE_RECALCULATE –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å–ª–æ–≤–∏–π:
# 1. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω --no-force-recalculate, —Ç–æ –≤—ã–∫–ª—é—á–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Å—á–µ—Ç
# 2. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω --force-recalculate, —Ç–æ –≤–∫–ª—é—á–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Å—á–µ—Ç
# 3. –ï—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∏–∑ PyCharm –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Ç–æ –≤–∫–ª—é—á–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Å—á–µ—Ç
# 4. –í –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö - –≤—ã–∫–ª—é—á–µ–Ω
if args.no_force_recalculate:
    FORCE_RECALCULATE = False
elif args.force_recalculate:
    FORCE_RECALCULATE = True
elif is_running_in_pycharm() and len(sys.argv) == 1:
    FORCE_RECALCULATE = True
    print("–ó–∞–ø—É—Å–∫ –∏–∑ PyCharm –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞")
else:
    FORCE_RECALCULATE = False

CACHE_DIR = os.path.join(tempfile.gettempdir(), "wiki_cache")
CACHE_FILE = os.path.join(CACHE_DIR, f"cache_{QUERY_ID}.pkl")

if USE_CACHE and not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

# === –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ===
cache = {}
if USE_CACHE and os.path.exists(CACHE_FILE) and not FORCE_RECALCULATE:
    try:
        with open(CACHE_FILE, 'rb') as f:
            cache = pickle.load(f)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞: {e}")
        cache = {}
else:
    # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞, –æ—á–∏—â–∞–µ–º –∫–µ—à
    if FORCE_RECALCULATE:
        log_message = "‚ö†Ô∏è –í–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞. –ö–µ—à –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è."
        print(log_message)
        if os.path.exists(CACHE_FILE):
            try:
                os.remove(CACHE_FILE)
                print(f"–£–¥–∞–ª–µ–Ω —Ñ–∞–π–ª –∫–µ—à–∞: {CACHE_FILE}")
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª –∫–µ—à–∞: {e}")
        cache = {}


def save_cache():
    if USE_CACHE and not FORCE_RECALCULATE:
        try:
            with open(CACHE_FILE, 'wb') as f:
                pickle.dump(cache, f)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à–∞: {e}")


# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
def log(message, level="INFO"):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{timestamp}] [{level}] {message}"
    with open(LOGFILE, "a", encoding="utf-8") as f:
        f.write(log_entry + "\n")
    print(log_entry)


def log_debug(message):
    if VERBOSE:
        log(message, "DEBUG")


# === –°–µ—Å—Å–∏–∏ –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è ===
# –°–æ–∑–¥–∞–µ–º –æ–±—â—É—é —Å–µ—Å—Å–∏—é –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
wikipedia_session = requests.Session()
commons_session = requests.Session()

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
user_agent = f'VoyagerimBot/1.0 (User:Voyagerim; quarry-id:{QUERY_ID})'
wikipedia_session.headers.update({'User-Agent': user_agent})
commons_session.headers.update({'User-Agent': user_agent})


# === –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–≥–Ω–∞–ª–∞ Ctrl+C –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è ===
def setup_signal_handler():
    def signal_handler(sig, frame):
        log("‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–µ—à –∏ –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É...")
        save_cache()
        log("–ö–µ—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
        sys.exit(1)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)


# === –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ Quarry ===
def run_quarry_query(query_id):
    log("‚ñ∂ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ Quarry...")
    tsv_url = f"https://quarry.wmcloud.org/query/{query_id}/result/latest/0/tsv"
    try:
        with tqdm(total=1, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö Quarry", unit="–∑–∞–ø—Ä–æ—Å") as pbar:
            r = requests.get(tsv_url)
            r.raise_for_status()
            quarry_rows = list(csv.DictReader(StringIO(r.text), delimiter="\t"))
            pbar.update(1)
        log(f"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(quarry_rows)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö")
        return quarry_rows
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ TSV: {e}", "ERROR")
        return None


# === –†–∞—Å—á—ë—Ç readable prose size ===
# –£–¥–∞–ª—è–µ–º –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä lru_cache, —á—Ç–æ–±—ã –Ω–µ –∫–µ—à–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–º—è—Ç–∏
# –ø—Ä–∏ –≤–∫–ª—é—á–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞
def get_prose_size(title):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞
    cache_key = f"prose_size:{title}"
    if USE_CACHE and not FORCE_RECALCULATE and cache_key in cache:
        log_debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π prose_size –¥–ª—è {title}: {cache[cache_key]}")
        return cache[cache_key]

    try:
        log_debug(f"–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ readable prose size –¥–ª—è '{title}'")
        r = wikipedia_session.get(WIKIPEDIA_API, params={
            "action": "parse",
            "page": title,
            "prop": "text",
            "format": "json",
            "formatversion": "2"
        })
        r.raise_for_status()
        data = r.json()

        if "error" in data:
            log_debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è '{title}': {data['error'].get('info', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            return 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞
        if "parse" not in data or "text" not in data["parse"]:
            log_debug(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è '{title}': –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç parse.text")
            return 0

        html = data["parse"]["text"]
        if not html:
            log_debug(f"–ü—É—Å—Ç–æ–π HTML –¥–ª—è '{title}'")
            return 0

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º HTML —Å BeautifulSoup
        soup = BeautifulSoup(html, "html.parser")

        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        for tag in soup.select('table, .navbox, .reference, script, style, .mw-editsection, .noprint'):
            tag.decompose()

        # –ò—â–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏ —Å—á–∏—Ç–∞–µ–º –∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä
        paragraphs = soup.find_all("p")
        if not paragraphs:
            log_debug(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ '{title}'")
            return 0

        text = " ".join(p.get_text().strip() for p in paragraphs)
        result = len(text)

        log_debug(f"–í—ã—á–∏—Å–ª–µ–Ω readable prose size –¥–ª—è '{title}': {result} —Å–∏–º–≤–æ–ª–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞
        if USE_CACHE and not FORCE_RECALCULATE:
            cache[cache_key] = result
            log_debug(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫–µ—à prose_size –¥–ª—è {title}: {result}")

        return result
    except Exception as e:
        log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ prose_size –¥–ª—è {title}: {e}", "WARNING")
        import traceback
        log_debug(f"–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")
        return 0


# === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç—å–∏ ===
# –¢–∞–∫–∂–µ —É–±–∏—Ä–∞–µ–º lru_cache –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç—å–∏ –ø—Ä–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–º –ø–µ—Ä–µ—Å—á–µ—Ç–µ
def get_quality(title):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞
    cache_key = f"quality:{title}"
    if USE_CACHE and not FORCE_RECALCULATE and cache_key in cache:
        return cache[cache_key]

    try:
        r = wikipedia_session.get(WIKIPEDIA_API, params={
            "action": "parse",
            "page": title,
            "prop": "wikitext",
            "format": "json",
            "formatversion": "2"
        })
        r.raise_for_status()
        data = r.json()
        wikitext = data.get("parse", {}).get("wikitext", "").lower()

        # –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —à–∞–±–ª–æ–Ω–æ–≤ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        result = ""
        if re.search(r"\{\{\s*(subst:\s*)?–∏–∑–±—Ä–∞–Ω–Ω–∞—è\s+—Å—Ç–∞—Ç—å—è(\|[^}]*)?}}", wikitext):
            result = "featured_article"
        elif re.search(r"\{\{\s*(subst:\s*)?–∏–∑–±—Ä–∞–Ω–Ω—ã–π\s+—Å–ø–∏—Å–æ–∫(\|[^}]*)?}}", wikitext):
            result = "featured_list"
        elif re.search(r"\{\{\s*(subst:\s*)?–∏–∑–±—Ä–∞–Ω–Ω—ã–π\s+—Å–ø–∏—Å–æ–∫\s+–∏–ª–∏\s+–ø–æ—Ä—Ç–∞–ª(\|[^}]*)?}}", wikitext):
            if re.search(r"—Ç–∏–ø\s*=\s*—Å–ø–∏—Å–æ–∫", wikitext):
                result = "featured_list"
        elif re.search(r"\{\{\s*(subst:\s*)?—Ö–æ—Ä–æ—à–∞—è\s+—Å—Ç–∞—Ç—å—è(\|[^}]*)?}}", wikitext):
            result = "good"
        elif re.search(r"\{\{\s*(subst:\s*)?–¥–æ–±—Ä–æ—Ç–Ω–∞—è\s+—Å—Ç–∞—Ç—å—è(\|[^}]*)?}}", wikitext):
            result = "b"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞
        if USE_CACHE and not FORCE_RECALCULATE:
            cache[cache_key] = result

        return result
    except Exception as e:
        log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è {title}: {e}", "WARNING")
        return ""


# === –ü–∞–∫–µ—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
def batch_get_wikitext(titles):
    """–ü–æ–ª—É—á–∞–µ—Ç wikitext –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å"""
    if not titles:
        return {}

    batch_size = 50  # API –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
    results = {}

    for i in range(0, len(titles), batch_size):
        batch = titles[i:i + batch_size]
        try:
            r = wikipedia_session.get(WIKIPEDIA_API, params={
                "action": "query",
                "titles": "|".join(batch),
                "prop": "revisions",
                "rvprop": "content",
                "rvslots": "main",
                "format": "json"
            })
            r.raise_for_status()
            data = r.json()

            if "query" in data and "pages" in data["query"]:
                for page_id, page_data in data["query"]["pages"].items():
                    if "revisions" in page_data and page_data["revisions"]:
                        title = page_data.get("title", "")
                        content = page_data["revisions"][0].get("slots", {}).get("main", {}).get("*", "")
                        results[title] = content
        except Exception as e:
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: {e}", "WARNING")

    return results


# === –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å Commons ===
def load_existing_data():
    log("‚ñ∂ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å Commons...")
    existing_map = {}
    try:
        with tqdm(total=1, desc="–ó–∞–≥—Ä—É–∑–∫–∞ —Å Commons", unit="–∑–∞–ø—Ä–æ—Å") as pbar:
            r = commons_session.get(COMMONS_API, params={
                "action": "query",
                "titles": COMMONS_DATA_TITLE,
                "prop": "revisions",
                "rvslots": "main",
                "rvprop": "content",
                "format": "json"
            })
            r.raise_for_status()
            pages = r.json()["query"]["pages"]
            page_id = next(iter(pages))
            pbar.update(1)

        if int(page_id) < 0:
            log("‚ÑπÔ∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Commons –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è.")
            return {}

        content = pages[page_id].get("revisions", [{}])[0].get("slots", {}).get("main", {}).get("*", "")
        if not content.strip():
            return {}

        json_data = json.loads(content)
        if "data" in json_data and isinstance(json_data["data"], list):
            field_names = [f["name"] for f in json_data.get("schema", {}).get("fields", [])]

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            with tqdm(total=len(json_data["data"]), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π", unit="–∑–∞–ø–∏—Å—å") as pbar:
                for row_data in json_data["data"]:
                    if isinstance(row_data, list) and len(row_data) == len(field_names):
                        row_obj = {field_names[i]: row_data[i] for i in range(len(field_names))}
                        existing_map[row_obj["page_title"]] = row_obj
                    pbar.update(1)

            log(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(existing_map)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π")
    except Exception as e:
        log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏/—Ä–∞–∑–±–æ—Ä–∞ Commons: {e}", "WARNING")
    return existing_map


# === –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Commons ===
def upload_data_to_commons(session, csrf_token, updated_rows):
    log(f"‚ñ∂ –ó–∞–≥—Ä—É–∂–∞–µ–º {len(updated_rows)} –∑–∞–ø–∏—Å–µ–π –Ω–∞ Commons...")

    schema_fields = [
        {"name": "page_title", "type": "string"},
        {"name": "created", "type": "string"},
        {"name": "size", "type": "number"},
        {"name": "prose_size", "type": "number"},
        {"name": "redlinks", "type": "number"},
        {"name": "is_disambiguation", "type": "number"},
        {"name": "comment", "type": "string"},
        {"name": "quality", "type": "string"}
    ]

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö
    tabular_data = []
    with tqdm(total=len(updated_rows), desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏", unit="–∑–∞–ø–∏—Å—å") as pbar:
        for row in updated_rows:
            tabular_data.append([
                row["page_title"],
                row["created"],
                int(float(row["size"])),
                int(row["prose_size"]),
                int(float(row["redlinks"])),
                int(row["is_disambiguation"]),
                row["comment"],
                row["quality"]
            ])
            pbar.update(1)

    upload_data = {
        "license": "CC0-1.0",
        "description": {"en": "Articles created by Voyagerim", "ru": "–°—Ç–∞—Ç—å–∏, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —É—á–∞—Å—Ç–Ω–∏–∫–æ–º Voyagerim"},
        "sources": "Imported from Quarry query results",
        "schema": {"fields": schema_fields},
        "data": tabular_data
    }

    # –ü—Ä–æ–≥—Ä–µ—Å—Å –ø—Ä–∏ —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
    with tqdm(total=1, desc="–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è JSON", unit="—Ñ–∞–π–ª") as pbar:
        upload_text = json.dumps(upload_data, ensure_ascii=False)
        pbar.update(1)

    if TEST_MODE:
        log("üß™ –¢–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º: –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞")
        log_debug(json.dumps(tabular_data[:3], ensure_ascii=False, indent=2))
        return True

    edit_params = {
        "action": "edit",
        "title": COMMONS_DATA_TITLE,
        "text": upload_text,
        "token": csrf_token,
        "format": "json",
        "summary": SUMMARY,
        "contentmodel": "Tabular.JsonConfig"
    }

    try:
        with tqdm(total=1, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Commons", unit="–∑–∞–ø—Ä–æ—Å") as pbar:
            r = session.post(COMMONS_API, data=edit_params)
            r.raise_for_status()
            response = r.json()
            pbar.update(1)

        if "edit" in response and response["edit"].get("result") == "Success":
            log(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(tabular_data)} –∑–∞–ø–∏—Å–µ–π!")
            log(f"üîó https://commons.wikimedia.org/wiki/{COMMONS_DATA_TITLE}")
            return True
        else:
            log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {json.dumps(response, indent=2)}", "ERROR")
            return False
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ Commons: {e}", "ERROR")
        return False


# === –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ Commons ===
def authenticate_commons():
    log("‚ñ∂ –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ Commons...")
    session = commons_session
    try:
        r1 = session.get(COMMONS_API, params={"action": "query", "meta": "tokens", "type": "login", "format": "json"})
        r1.raise_for_status()
        login_token = r1.json()["query"]["tokens"]["logintoken"]

        login_response = session.post(COMMONS_API, data={
            "action": "login",
            "lgname": USERNAME,
            "lgpassword": PASSWORD,
            "lgtoken": login_token,
            "format": "json"
        })
        login_response.raise_for_status()
        if login_response.json().get("login", {}).get("result") != "Success":
            log("‚ùå –û—à–∏–±–∫–∞ –≤—Ö–æ–¥–∞", "ERROR")
            return None, None

        r2 = session.get(COMMONS_API, params={"action": "query", "meta": "tokens", "format": "json"})
        r2.raise_for_status()
        csrf_token = r2.json()["query"]["tokens"]["csrftoken"]

        log("‚úÖ –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞")
        return session, csrf_token
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}", "ERROR")
        return None, None


# === –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π ===
def process_article_data(quarry_rows, existing_map):
    log("‚ñ∂ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–µ–π...")

    # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞, –≤—ã–≤–æ–¥–∏–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
    if FORCE_RECALCULATE:
        log("‚ö†Ô∏è –í–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞ prose_size. –ö–µ—à –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.")

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø–æ–ª—É—á–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –≤—Å–µ—Ö —Å—Ç–∞—Ç–µ–π –ø–∞–∫–µ—Ç–∞–º–∏
    def prepare_quality_data():
        all_titles = [row["page_title"] for row in quarry_rows]

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–ª–æ–∫–∏ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        batch_size = min(50, MAX_WORKERS * 5)  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –¥–ª—è API
        batch_titles = []
        total_batches = (len(all_titles) + batch_size - 1) // batch_size

        log(f"–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(all_titles)} —Å—Ç–∞—Ç–µ–π...")
        with tqdm(total=total_batches, desc="–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞", unit="–ø–∞–∫–µ—Ç") as pbar:
            for i in range(0, len(all_titles), batch_size):
                batch_titles = all_titles[i:i + batch_size]
                # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤–∏–∫–∏—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
                batch_get_wikitext(batch_titles)
                pbar.update(1)

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    prepare_quality_data()

    def fetch_row_data(row):
        title = row["page_title"]
        comment = existing_map.get(title, {}).get("comment", "")

        # –ü–æ–ª—É—á–∞–µ–º prose_size —Å –æ—Ç–º–µ—Ç–∫–æ–π –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        start_time = time.time()
        prose_size = get_prose_size(title)
        elapsed = time.time() - start_time

        # –í —Ä–µ–∂–∏–º–µ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–º —Ä–∞—Å—á–µ—Ç–µ
        if FORCE_RECALCULATE:
            log_debug(f"–ü–ï–†–ï–°–ß–ï–¢ prose_size={prose_size} –¥–ª—è '{title}' –∑–∞ {elapsed:.3f} —Å–µ–∫")
        else:
            log_debug(f"–ü–æ–ª—É—á–µ–Ω prose_size={prose_size} –¥–ª—è '{title}' –∑–∞ {elapsed:.3f} —Å–µ–∫")

        is_disambiguation = row.get("is_disambiguation", 0)
        quality = get_quality(title)

        return {
            "page_title": title,
            "created": row["created"],
            "size": row.get("size", 0),
            "prose_size": prose_size,
            "redlinks": row.get("redlinks", 0),
            "is_disambiguation": is_disambiguation,
            "comment": comment,
            "quality": quality
        }

    updated_rows = []
    start_time = time.time()

    log(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(quarry_rows)} —Å—Ç–∞—Ç–µ–π...")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(fetch_row_data, row) for row in quarry_rows]

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π progress bar —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        with tqdm(total=len(futures), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç–µ–π", unit="—Å—Ç–∞—Ç—å—è",
                  bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]") as pbar:
            for f in as_completed(futures):
                try:
                    result = f.result()
                    updated_rows.append(result)

                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É prose_size
                    if len(updated_rows) % 10 == 0:
                        current_speed = len(updated_rows) / (time.time() - start_time)
                        avg_prose_size = sum(row["prose_size"] for row in updated_rows) / len(
                            updated_rows) if updated_rows else 0
                        pbar.set_postfix(—Å–∫–æ—Ä–æ—Å—Ç—å=f"{current_speed:.2f} —Å—Ç/—Å–µ–∫", —Å—Ä–µ–¥_—Ä–∞–∑–º–µ—Ä=f"{avg_prose_size:.1f}")
                except Exception as e:
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}", "WARNING")
                    import traceback
                    log_debug(f"–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")
                pbar.update(1)

    elapsed = time.time() - start_time
    # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ prose_size
    avg_prose = sum(row["prose_size"] for row in updated_rows) / len(updated_rows) if updated_rows else 0
    max_prose = max((row["prose_size"] for row in updated_rows), default=0)
    min_prose = min((row["prose_size"] for row in updated_rows if row["prose_size"] > 0), default=0)
    zero_prose = sum(1 for row in updated_rows if row["prose_size"] == 0)

    log(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(updated_rows)} —Å—Ç–∞—Ç–µ–π –∑–∞ {elapsed:.2f} —Å–µ–∫—É–Ω–¥ ({len(updated_rows) / elapsed:.2f} —Å—Ç–∞—Ç–µ–π/—Å–µ–∫)")
    log(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ prose_size: —Å—Ä–µ–¥–Ω–µ–µ={avg_prose:.1f}, –º–∏–Ω={min_prose}, –º–∞–∫—Å={max_prose}, –Ω—É–ª–∏={zero_prose}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–µ—à –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏, –µ—Å–ª–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ—Å—á–µ—Ç–∞
    if not FORCE_RECALCULATE:
        save_cache()

    return updated_rows


# === –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
def main():
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    setup_signal_handler()

    log(f"=== –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ {os.path.basename(__file__)} ===")
    log(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å Quarry: {QUERY_ID}")
    log(f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤: {MAX_WORKERS}")
    log(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–µ—à–∞: {USE_CACHE}")
    log(f"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Å—á–µ—Ç: {FORCE_RECALCULATE}")

    if TEST_MODE:
        log("üß™ –ó–∞–ø—É—Å–∫ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
    total_steps = 4  # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ Quarry, –∑–∞–≥—Ä—É–∑–∫–∞ —Å Commons, –æ–±—Ä–∞–±–æ—Ç–∫–∞, –≤—ã–≥—Ä—É–∑–∫–∞ –Ω–∞ Commons
    with tqdm(total=total_steps, desc="–û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å", position=0,
              bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]") as main_pbar:

        quarry_rows = run_quarry_query(QUERY_ID)
        main_pbar.update(1)

        if not quarry_rows:
            log("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Quarry. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.", "ERROR")
            return 1

        existing_map = load_existing_data()
        main_pbar.update(1)

        updated_rows = process_article_data(quarry_rows, existing_map)
        main_pbar.update(1)

        if not updated_rows:
            log("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.", "ERROR")
            return 1

        session, csrf_token = authenticate_commons()
        if not session or not csrf_token:
            return 1

        result = upload_data_to_commons(session, csrf_token, updated_rows)
        main_pbar.update(1)

    log("=== –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã —Å–∫—Ä–∏–ø—Ç–∞ ===")
    return 0 if result else 1


if __name__ == "__main__":
    sys.exit(main())